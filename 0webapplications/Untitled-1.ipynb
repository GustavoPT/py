{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c8cf7745fd60c9bf30273b9b91479f0ccd4ed07ec75b7241c5b921a1555acd41"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re \n",
    "import string \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# Make a request to the website\n",
    "r = requests.get('https://bola.kompas.com/')\n",
    "# Create an object to parse the HTML format\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# Retrieve all popular news links (Fig. 1)\n",
    "link = []\n",
    "for i in soup.find('div', {'class':'most__wrap'}).find_all('a'):\n",
    "    i['href'] = i['href'] + '?page=all'\n",
    "    link.append(i['href'])\n",
    "# For each link, we retrieve paragraphs from it, combine each paragraph as one string, and save it to documents (Fig. 2)\n",
    "documents = []\n",
    "for i in link:\n",
    "    # Make a request to the link\n",
    "    r = requests.get(i)\n",
    "  \n",
    "    # Initialize BeautifulSoup object to parse the content \n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "  \n",
    "    # Retrieve all paragraphs and combine it as one\n",
    "    sen = []\n",
    "    for i in soup.find('div', {'class':'read__content'}).find_all('p'):\n",
    "        sen.append(i.text)\n",
    "  \n",
    "    # Add the combined paragraphs to documents\n",
    "    documents.append(' '.join(sen))\n",
    "\n",
    "documents_clean = []\n",
    "for d in documents:\n",
    "    # Remove Unicode\n",
    "    document_test = re.sub(r'[^\\x00-\\x7F]+', ' ', d)\n",
    "    # Remove Mentions\n",
    "    document_test = re.sub(r'@\\w+', '', document_test)\n",
    "    # Lowercase the document\n",
    "    document_test = document_test.lower()\n",
    "    # Remove punctuations\n",
    "    document_test = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', document_test)\n",
    "    # Lowercase the numbers\n",
    "    document_test = re.sub(r'[0-9]', '', document_test)\n",
    "    # Remove the doubled space\n",
    "    document_test = re.sub(r'\\s{2,}', ' ', document_test)\n",
    "    documents_clean.append(document_test)\n",
    "\n",
    "# Instantiate a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# It fits the data and transform it as a vector\n",
    "X = vectorizer.fit_transform(documents_clean)\n",
    "# Convert the X as transposed matrix\n",
    "X = X.T.toarray()\n",
    "# Create a DataFrame and set the vocabulary as the index\n",
    "df = pd.DataFrame(X, index=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "query: barcelona\nBerikut artikel dengan nilai cosine similarity tertinggi: \nNilai Similaritas: 0.48651642675369927\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fc03d67dea27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Add The Query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mq1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'barcelona'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mget_similar_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-fc03d67dea27>\u001b[0m in \u001b[0;36mget_similar_articles\u001b[1;34m(q, df)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nilai Similaritas:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Add The Query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "def get_similar_articles(q, df):\n",
    "  print(\"query:\", q)\n",
    "  print(\"Berikut artikel dengan nilai cosine similarity tertinggi: \")\n",
    "  # Convert the query become a vector\n",
    "  q = [q]\n",
    "  q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "  sim = {}\n",
    "  # Calculate the similarity\n",
    "  for i in range(10):\n",
    "    sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)\n",
    "  \n",
    "  # Sort the values \n",
    "  sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
    "  # Print the articles and their similarity values\n",
    "  for k, v in sim_sorted:\n",
    "    if v != 0.0:\n",
    "      print(\"Nilai Similaritas:\", v)\n",
    "      print(docs[k])\n",
    "      print()\n",
    "# Add The Query\n",
    "q1 = 'barcelona'\n",
    "get_similar_articles(q1, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}